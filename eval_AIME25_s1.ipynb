{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072bc71e1cfa4dcda08625c345036923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/830 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 19:00:19 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-14 19:00:23 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 02-14 19:00:23 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 02-14 19:00:23 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='simplescaling/s1-32B', speculative_config=None, tokenizer='simplescaling/s1-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=simplescaling/s1-32B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfb2ad576ec490f8ea5f9808891f07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-14 19:00:24 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-14 19:00:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 02-14 19:00:24 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:00:24 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:00:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:00:24 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:00:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:00:24 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:00:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 02-14 19:00:26 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-14 19:00:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:00:26 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-14 19:00:26 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-14 19:00:26 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:00:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-14 19:00:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-14 19:00:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 02-14 19:00:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m WARNING 02-14 19:00:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-14 19:00:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-14 19:00:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-14 19:00:26 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9e226b3c70>, local_subscribe_port=35335, remote_subscribe_port=None)\n",
      "INFO 02-14 19:00:26 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:00:26 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "INFO 02-14 19:00:26 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "INFO 02-14 19:00:26 model_runner.py:1072] Starting to load model simplescaling/s1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:00:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:00:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:00:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-14 19:00:26 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984783c012214695824639870b494672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da642ce0003438091bb04469491e333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944499854f5e4f199203b4a1a865a2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5dbd01bcf8406ab875c7c1b5ed7a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eaa5f37e9548b1b1ad76100756a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00029.safetensors:   0%|          | 0.00/4.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b8aaf16224242b39ba276e91f5eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ce51b6c66a4f2cac40efb7c0a316d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4978db20117b48fd8ec6d8fde506c134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e289b7b190fa4971bf6c5d1b83331490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f606300b83f4b68b2359cefce7caaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bbdedbb9874729a63300107d1cf1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85bb59bdb374297a9e6d490ee341aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0143ba30eb4fa5abffc8bfffeab3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fa83c010da4b6b8aec08f9a031292b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00014-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdc51f62f254a74a76a87881baf041a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00015-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442d593bc4ef47ebbf3481797b68b2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00016-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0b3d7c1a584e60a9528ec51e21d058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00017-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d79ab4cdef49aeb22453c63a679618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9435e1c2a645668b6822dc82d4eee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd911ab4831440d8b398cc9813d822c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e878781f5c94c429a40df7c3aa8873b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9094df2782a543cb8216ff2965b48286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fccbfd4f29a4749b73c156e98b59b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f315e7b5f8b428d91ada7d609e9a488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cc2b38d251454f87f9ab59def6ce2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45702737675f44d394042f2106c689d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00029.safetensors:   0%|          | 0.00/4.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a3cea365024eec951034819af2e041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00027-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98ee4f11ae74f8daa85e729071bd7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00028-of-00029.safetensors:   0%|          | 0.00/4.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2434f74ef73e468998dd7e0eddb8e116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00029-of-00029.safetensors:   0%|          | 0.00/3.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba57894e32224dc58bd3ba5b05cf2246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/63.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac67c6fb2ddc4fb2beeb64d5dae607f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/29 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-14 19:14:46 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:14:46 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:14:46 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:14:46 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:14:55 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=15.98GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.09GiB non_torch_memory=0.69GiB kv_cache_size=24.08GiB gpu_memory_utilization=0.90\n",
      "INFO 02-14 19:14:55 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.17GiB non_torch_memory=0.77GiB kv_cache_size=24.01GiB gpu_memory_utilization=0.90\n",
      "INFO 02-14 19:14:55 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.17GiB non_torch_memory=0.77GiB kv_cache_size=24.01GiB gpu_memory_utilization=0.90\n",
      "INFO 02-14 19:14:55 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.20GiB non_torch_memory=0.80GiB kv_cache_size=23.97GiB gpu_memory_utilization=0.90\n",
      "INFO 02-14 19:14:55 distributed_gpu_executor.py:57] # GPU blocks: 24542, # CPU blocks: 4096\n",
      "INFO 02-14 19:14:55 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 11.98x\n",
      "INFO 02-14 19:15:00 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-14 19:15:00 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:15:00 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869989)\u001b[0;0m INFO 02-14 19:15:19 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "INFO 02-14 19:15:19 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869988)\u001b[0;0m INFO 02-14 19:15:19 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2869990)\u001b[0;0m INFO 02-14 19:15:19 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "model = LLM(\n",
    "    \"simplescaling/s1-32B\",\n",
    "    tensor_parallel_size=4,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(\"simplescaling/s1-32B\")\n",
    "\n",
    "stop_token_ids = tok(\"<|im_end|>\")[\"input_ids\"]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=32768,\n",
    "    min_tokens=0,\n",
    "    stop_token_ids=stop_token_ids,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:26<00:00, 86.52s/it, est. speed input: 0.66 toks/s, output: 33.88 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['70']\n",
      "correct: 1, in 1 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:42<00:00, 162.22s/it, est. speed input: 1.15 toks/s, output: 33.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['912']\n",
      "correct: 1, in 2 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:04<00:00, 184.13s/it, est. speed input: 0.77 toks/s, output: 33.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['646']\n",
      "correct: 1, in 3 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:21<00:00, 141.91s/it, est. speed input: 0.59 toks/s, output: 33.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['117']\n",
      "correct: 2, in 4 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:50<00:00, 170.65s/it, est. speed input: 0.65 toks/s, output: 33.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['873']\n",
      "correct: 2, in 5 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:26<00:00, 146.13s/it, est. speed input: 0.75 toks/s, output: 33.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['504']\n",
      "correct: 3, in 6 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:10<00:00, 190.21s/it, est. speed input: 0.90 toks/s, output: 33.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['7']\n",
      "correct: 3, in 7 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:14<00:00, 194.48s/it, est. speed input: 0.76 toks/s, output: 33.57 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['37']\n",
      "correct: 3, in 8 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:18<00:00, 198.21s/it, est. speed input: 0.67 toks/s, output: 33.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['62']\n",
      "correct: 4, in 9 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:34<00:00, 154.30s/it, est. speed input: 3.48 toks/s, output: 33.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['62']\n",
      "correct: 4, in 10 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:54<00:00, 234.82s/it, est. speed input: 3.66 toks/s, output: 32.87 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['274']\n",
      "correct: 4, in 11 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:00<00:00, 180.64s/it, est. speed input: 0.76 toks/s, output: 33.58 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['588']\n",
      "correct: 4, in 12 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:24<00:00, 144.98s/it, est. speed input: 0.74 toks/s, output: 33.67 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['128']\n",
      "correct: 4, in 13 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:34<00:00, 214.67s/it, est. speed input: 0.75 toks/s, output: 33.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['194']\n",
      "correct: 4, in 14 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:54<00:00, 174.45s/it, est. speed input: 0.60 toks/s, output: 33.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['001']\n",
      "correct: 4, in 15 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:28<00:00, 148.92s/it, est. speed input: 0.78 toks/s, output: 33.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['468']\n",
      "correct: 5, in 16 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:48<00:00, 108.93s/it, est. speed input: 0.60 toks/s, output: 33.72 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['49']\n",
      "correct: 6, in 17 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:35<00:00, 155.18s/it, est. speed input: 1.63 toks/s, output: 33.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['56']\n",
      "correct: 6, in 18 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:06<00:00, 186.61s/it, est. speed input: 1.19 toks/s, output: 33.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['106']\n",
      "correct: 7, in 19 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:29<00:00, 149.62s/it, est. speed input: 3.46 toks/s, output: 33.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['516']\n",
      "correct: 7, in 20 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:38<00:00, 158.52s/it, est. speed input: 3.84 toks/s, output: 33.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['82']\n",
      "correct: 7, in 21 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:30<00:00, 270.27s/it, est. speed input: 0.44 toks/s, output: 32.50 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['60927']\n",
      "correct: 7, in 22 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:03<00:00, 243.73s/it, est. speed input: 1.16 toks/s, output: 33.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['733']\n",
      "correct: 7, in 23 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:52<00:00, 172.44s/it, est. speed input: 0.63 toks/s, output: 33.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['149']\n",
      "correct: 8, in 24 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:52<00:00, 172.98s/it, est. speed input: 0.55 toks/s, output: 33.60 toks/s]\n"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode characters in position 6274-6284: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m o \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(prompt, sampling_params\u001b[38;5;241m=\u001b[39msampling_params)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput.log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(pattern, o[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode characters in position 6274-6284: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/ssdscratch/byuan48/math_eval/data/AIME25.jsonl\")\n",
    "correct = 0\n",
    "for i in range(len(dataset['train'])):\n",
    "    prompt = dataset['train']['problem'][i]\n",
    "    answer = dataset['train']['answer'][i]\n",
    "    prompt = \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\" + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    o = model.generate(prompt, sampling_params=sampling_params)\n",
    "    with open('output.log', 'w') as f:\n",
    "        f.write(o[0].outputs[0].text)\n",
    "    pattern = r\"\\\\boxed\\{(\\d+)\\}\"\n",
    "    matches = re.findall(pattern, o[0].outputs[0].text)\n",
    "    print(f'matches: {matches}')\n",
    "    if len(set(matches)) != 1:\n",
    "        print(f'matches: {matches} not unique')\n",
    "        print(f'current answer: {answer}')\n",
    "    # remove the leading 0s\n",
    "    matches[-1] = matches[-1].lstrip('0')\n",
    "    if matches[-1] == str(answer):\n",
    "        correct += 1\n",
    "    print(f'correct: {correct}, in {i+1} questions')\n",
    "\n",
    "print(f'correct: {correct}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:03<00:00, 183.31s/it, est. speed input: 0.52 toks/s, output: 33.62 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['758']\n",
      "correct: 0, in 25 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:46<00:00, 166.07s/it, est. speed input: 0.51 toks/s, output: 33.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['12']\n",
      "correct: 0, in 26 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:07<00:00, 187.53s/it, est. speed input: 1.14 toks/s, output: 33.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['71']\n",
      "correct: 0, in 27 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:12<00:00, 252.74s/it, est. speed input: 0.56 toks/s, output: 33.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['0']\n",
      "correct: 0, in 28 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:59<00:00, 239.72s/it, est. speed input: 0.50 toks/s, output: 33.45 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['168']\n",
      "correct: 0, in 29 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:19<00:00, 199.84s/it, est. speed input: 0.50 toks/s, output: 33.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['188']\n",
      "correct: 0, in 30 questions\n",
      "correct: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/ssdscratch/byuan48/math_eval/data/AIME25.jsonl\")\n",
    "correct = 0\n",
    "for i in range(24, len(dataset['train'])):\n",
    "    prompt = dataset['train']['problem'][i]\n",
    "    answer = dataset['train']['answer'][i]\n",
    "    prompt = \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\" + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    o = model.generate(prompt, sampling_params=sampling_params)\n",
    "    pattern = r\"\\\\boxed\\{(\\d+)\\}\"\n",
    "    matches = re.findall(pattern, o[0].outputs[0].text)\n",
    "    print(f'matches: {matches}')\n",
    "    if len(set(matches)) != 1:\n",
    "        print(f'matches: {matches} not unique')\n",
    "        print(f'current answer: {answer}')\n",
    "    # remove the leading 0s\n",
    "    matches[-1] = matches[-1].lstrip('0')\n",
    "    if matches[-1] == str(answer):\n",
    "        correct += 1\n",
    "    print(f'correct: {correct}, in {i+1} questions')\n",
    "\n",
    "print(f'correct: {correct}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
