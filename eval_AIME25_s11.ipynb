{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-17 20:31:06 config.py:1861] Downcasting torch.float32 to torch.float16.\n",
      "INFO 02-17 20:31:11 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 02-17 20:31:11 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 02-17 20:31:11 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='simplescaling/s1.1-32B', speculative_config=None, tokenizer='simplescaling/s1.1-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=simplescaling/s1.1-32B, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 02-17 20:31:11 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-17 20:31:11 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 02-17 20:31:11 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:11 selector.py:135] Using Flash Attention backend.\n",
      "INFO 02-17 20:31:11 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:11 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:31:11 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:31:11 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:31:11 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:31:13 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-17 20:31:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:13 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-17 20:31:13 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 02-17 20:31:13 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:31:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 02-17 20:31:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m WARNING 02-17 20:31:13 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m WARNING 02-17 20:31:13 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-17 20:31:13 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-17 20:31:13 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-17 20:31:13 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f824b91d580>, local_subscribe_port=60133, remote_subscribe_port=None)\n",
      "INFO 02-17 20:31:13 model_runner.py:1072] Starting to load model simplescaling/s1.1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:13 model_runner.py:1072] Starting to load model simplescaling/s1.1-32B...\n",
      "INFO 02-17 20:31:13 model_runner.py:1072] Starting to load model simplescaling/s1.1-32B...\n",
      "INFO 02-17 20:31:13 model_runner.py:1072] Starting to load model simplescaling/s1.1-32B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:31:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:31:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-17 20:31:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:31:14 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fd7a8c048c4b2485e07830a705b15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/29 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:32:52 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:32:53 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "INFO 02-17 20:32:53 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:32:54 model_runner.py:1077] Loading model weights took 15.3916 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:33:03 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.17GiB non_torch_memory=0.77GiB kv_cache_size=24.01GiB gpu_memory_utilization=0.90\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:33:03 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=15.98GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.09GiB non_torch_memory=0.69GiB kv_cache_size=24.08GiB gpu_memory_utilization=0.90\n",
      "INFO 02-17 20:33:03 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.17GiB non_torch_memory=0.77GiB kv_cache_size=24.01GiB gpu_memory_utilization=0.90\n",
      "INFO 02-17 20:33:03 worker.py:232] Memory profiling results: total_gpu_memory=47.43GiB initial_memory_usage=16.01GiB peak_torch_memory=17.92GiB memory_usage_post_profile=16.20GiB non_torch_memory=0.80GiB kv_cache_size=23.97GiB gpu_memory_utilization=0.90\n",
      "INFO 02-17 20:33:03 distributed_gpu_executor.py:57] # GPU blocks: 24542, # CPU blocks: 4096\n",
      "INFO 02-17 20:33:03 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 11.98x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:33:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-17 20:33:07 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-17 20:33:07 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623466)\u001b[0;0m INFO 02-17 20:33:26 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623467)\u001b[0;0m INFO 02-17 20:33:26 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "INFO 02-17 20:33:26 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1623465)\u001b[0;0m INFO 02-17 20:33:26 model_runner.py:1518] Graph capturing finished in 19 secs, took 0.76 GiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "model = LLM(\n",
    "    \"simplescaling/s1.1-32B\",\n",
    "    tensor_parallel_size=4,\n",
    ")\n",
    "tok = AutoTokenizer.from_pretrained(\"simplescaling/s1-32B\")\n",
    "\n",
    "stop_token_ids = tok(\"<|im_end|>\")[\"input_ids\"]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=32768,\n",
    "    min_tokens=0,\n",
    "    stop_token_ids=stop_token_ids,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:24<00:00, 144.90s/it, est. speed input: 0.39 toks/s, output: 19.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['70', '70']\n",
      "correct: 1, in 1 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [17:02<00:00, 1022.83s/it, est. speed input: 0.18 toks/s, output: 18.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['888', '888']\n",
      "correct: 1, in 2 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:56<00:00, 236.64s/it, est. speed input: 0.60 toks/s, output: 19.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['16', '16']\n",
      "correct: 2, in 3 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:19<00:00, 259.73s/it, est. speed input: 0.32 toks/s, output: 19.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['117', '117']\n",
      "correct: 3, in 4 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [12:59<00:00, 779.29s/it, est. speed input: 0.14 toks/s, output: 18.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['279', '279', '279']\n",
      "correct: 4, in 5 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [02:31<00:00, 151.46s/it, est. speed input: 0.73 toks/s, output: 19.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['504', '504']\n",
      "correct: 5, in 6 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [14:36<00:00, 876.30s/it, est. speed input: 0.20 toks/s, output: 18.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['701', '701']\n",
      "correct: 5, in 7 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [09:49<00:00, 589.97s/it, est. speed input: 0.25 toks/s, output: 19.05 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['77', '77']\n",
      "correct: 6, in 8 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [17:08<00:00, 1028.88s/it, est. speed input: 0.13 toks/s, output: 18.60 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['14', '14']\n",
      "correct: 6, in 9 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [25:31<00:00, 1531.97s/it, est. speed input: 0.35 toks/s, output: 18.19 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['56', '108', '56', '56']\n",
      "matches: ['56', '108', '56', '56'] not unique\n",
      "current answer: 81\n",
      "correct: 6, in 10 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [23:44<00:00, 1424.61s/it, est. speed input: 0.60 toks/s, output: 18.23 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['615', '615']\n",
      "correct: 6, in 11 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [17:19<00:00, 1039.47s/it, est. speed input: 0.13 toks/s, output: 18.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['52', '52']\n",
      "correct: 6, in 12 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [15:37<00:00, 937.76s/it, est. speed input: 0.12 toks/s, output: 18.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: []\n",
      "no matches found\n",
      "correct: 6, in 13 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [16:03<00:00, 963.21s/it, est. speed input: 0.17 toks/s, output: 18.64 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['47', '47']\n",
      "correct: 6, in 14 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [17:20<00:00, 1040.13s/it, est. speed input: 0.10 toks/s, output: 18.59 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['441', '441', '441', '441', '441']\n",
      "correct: 6, in 15 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:01<00:00, 241.16s/it, est. speed input: 0.48 toks/s, output: 19.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['468', '468', '468', '468']\n",
      "correct: 7, in 16 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [03:12<00:00, 192.86s/it, est. speed input: 0.34 toks/s, output: 19.54 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['49', '49', '49']\n",
      "correct: 8, in 17 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [19:01<00:00, 1141.47s/it, est. speed input: 0.22 toks/s, output: 18.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['16', '16']\n",
      "correct: 8, in 18 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [04:07<00:00, 247.79s/it, est. speed input: 0.90 toks/s, output: 19.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['106', '106']\n",
      "correct: 9, in 19 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [13:21<00:00, 801.90s/it, est. speed input: 0.65 toks/s, output: 18.76 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['360', '360']\n",
      "correct: 9, in 20 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [08:12<00:00, 492.05s/it, est. speed input: 1.24 toks/s, output: 19.14 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['293', '293', '293']\n",
      "correct: 10, in 21 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [10:39<00:00, 639.16s/it, est. speed input: 0.18 toks/s, output: 19.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['60671', '60671']\n",
      "correct: 10, in 22 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [19:23<00:00, 1163.29s/it, est. speed input: 0.24 toks/s, output: 18.47 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['600', '600']\n",
      "correct: 10, in 23 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [08:15<00:00, 495.71s/it, est. speed input: 0.22 toks/s, output: 19.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['149', '149']\n",
      "correct: 11, in 24 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [19:16<00:00, 1156.88s/it, est. speed input: 0.08 toks/s, output: 18.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['962', '962']\n",
      "correct: 11, in 25 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [12:52<00:00, 772.80s/it, est. speed input: 0.11 toks/s, output: 18.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['324', '324', '324']\n",
      "correct: 11, in 26 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [15:30<00:00, 930.45s/it, est. speed input: 0.23 toks/s, output: 18.68 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['19', '19']\n",
      "correct: 12, in 27 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [21:58<00:00, 1318.75s/it, est. speed input: 0.11 toks/s, output: 18.38 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['503', '503']\n",
      "correct: 12, in 28 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [12:08<00:00, 728.35s/it, est. speed input: 0.16 toks/s, output: 18.90 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['264', '264']\n",
      "correct: 12, in 29 questions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/ssdscratch/byuan48/math_eval/data/AIME25.jsonl\")\n",
    "correct = 0\n",
    "for i in range(len(dataset['train'])):\n",
    "    prompt = dataset['train']['problem'][i]\n",
    "    answer = dataset['train']['answer'][i]\n",
    "    prompt = \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\" + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    o = model.generate(prompt, sampling_params=sampling_params)\n",
    "    pattern = r\"\\\\boxed\\{(\\d+)\\}\"\n",
    "    matches = re.findall(pattern, o[0].outputs[0].text)\n",
    "    print(f'matches: {matches}')\n",
    "    if len(matches) == 0:\n",
    "        print(f'no matches found')\n",
    "    else:    \n",
    "        if len(set(matches)) != 1:\n",
    "            print(f'matches: {matches} not unique')\n",
    "            print(f'current answer: {answer}')\n",
    "        # remove the leading 0s\n",
    "        matches[-1] = matches[-1].lstrip('0')\n",
    "        if matches[-1] == str(answer):\n",
    "            correct += 1\n",
    "    print(f'correct: {correct}, in {i+1} questions')\n",
    "\n",
    "print(f'correct: {correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [15:13<00:00, 913.00s/it, est. speed input: 0.11 toks/s, output: 27.79 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches: ['188', '188']\n",
      "correct: 0, in 30 questions\n",
      "correct: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"/ssdscratch/byuan48/math_eval/data/AIME25.jsonl\")\n",
    "correct = 0\n",
    "for i in range(29,len(dataset['train'])):\n",
    "    prompt = dataset['train']['problem'][i]\n",
    "    answer = dataset['train']['answer'][i]\n",
    "    prompt = \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\" + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    o = model.generate(prompt, sampling_params=sampling_params)\n",
    "    pattern = r\"\\\\boxed\\{(\\d+)\\}\"\n",
    "    matches = re.findall(pattern, o[0].outputs[0].text)\n",
    "    print(f'matches: {matches}')\n",
    "    if len(matches) == 0:\n",
    "        print(f'no matches found')\n",
    "    else:    \n",
    "        if len(set(matches)) != 1:\n",
    "            print(f'matches: {matches} not unique')\n",
    "            print(f'current answer: {answer}')\n",
    "        # remove the leading 0s\n",
    "        matches[-1] = matches[-1].lstrip('0')\n",
    "        if matches[-1] == str(answer):\n",
    "            correct += 1\n",
    "    print(f'correct: {correct}, in {i+1} questions')\n",
    "\n",
    "print(f'correct: {correct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for o1-preview:\n",
    "1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
